# Function_Approximation

Q学習を関数近似した。φ=exp(-(x-μ)^2/2*σ^2)の基底関数を用いてQ=θ*φ のθをθ=θ+α(r+Qmax-Q)φ　で更新した。  
一次元でAgentがTargetの方向へ進むだけ。局所解に陥った時のために制限時間を設けた。  
Q値をグラフとして描画することで視覚的に学習状況を把握できる。  
μとσの値の調整は別にプログラムを作ってグラフを描画してみて関数の広がりと重なり具合を調整した。  
このパラメータの調整をニューラルネットワークを用いればできると思われる。そのようにしてできたのがDQNなのかなとおもってる。
